{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB1Jrjidd1yMjy+MBG69RM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alighazal/machine-learning-from-scratch/blob/main/Implementing_Transformers_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YfhpVGGF9gZR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8b6e38c4-b8cf-47e9-e471-9defc9c849c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-cd32c7c163e4>, line 163)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-cd32c7c163e4>\"\u001b[0;36m, line \u001b[0;32m163\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "\n",
        "# inpul embedding:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, seq_len, dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # create a matrix of shape (seq_len * d_model)\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "    positions = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp( torch.arange(  0, d_model, 2).float() * (-math.log(10000.0) / d_model ))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin( positions * div_term) ## TODO step through this\n",
        "    pe[:, 1::2] = torch.cos( positions * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).require_grad_(False)\n",
        "    return self.dropout(x)\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, eps: float = 10**-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "      mean = x.mean( dim = -1, keepDim=True)\n",
        "      std = x.std( dim = -1, keepDim=True)\n",
        "      return self.alpha * (x- mean) / (std + self.eps ) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_2(self.dropout(torch.relu( self.linear_1(x))))\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "  def __init__(self, d_model, h, dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    assert d_model % h == 0,  \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // h\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim = -1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "\n",
        "    query = query.view( query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    key = key.view( key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    value = value.view( value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, self.dropout)\n",
        "\n",
        "    x = x.transpose(1,2).contiguous().view( x.shape[0], -1, self.h * self.d_k )\n",
        "\n",
        "    return self.w_o(x)\n",
        "\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "  def __init__( self, dropout ):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout( sublayer( self.norm(x)) )\n",
        "\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList( [  ResidualConnection(dropout) for _ in range(2) ] )\n",
        "\n",
        "  def forward( self, x, src_mask ):\n",
        "    x = self.residual_connections[0]( x, lambda x : self.self_attention_block( x,x,x, src_mask) )\n",
        "    x = self.residual_connections[1]( x, self.feed_forward_block )\n",
        "    return x\n",
        "\n",
        "class Encoder( nn.Module):\n",
        "\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward(self,x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "\n",
        "class DecoderBlock( nn.Module ):\n",
        "  def __init__(self, self_attention_block: MultiHeadAttentionBlock,\n",
        "               cross_attendtion_block: MultiHeadAttentionBlock,\n",
        "               feed_forward_block: FeedForwardBlock,\n",
        "               dropout: float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attendtion_block = cross_attendtion_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList( [  ResidualConnection(dropout) for _ in range(3) ] )\n",
        "\n",
        "  def forward(self,x, encoder_output, src_mask, target_mask):\n",
        "    x = self.residual_connections[0]( x ,  lambda x: self.self_attention_block(x,x,x, target_mask) )\n",
        "    x = self.residual_connections[1]( x ,  lambda x: self.cross_attendtion_block(x,encoder_output,encoder_output, src_mask) )\n",
        "    x = self.residual_connections[2]( x ,  self.feed_forward_block )\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__( self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "  def forward( self, x, encoder_output, src_mask, target_mask  ):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, target_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LZqDmqMsEQY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}